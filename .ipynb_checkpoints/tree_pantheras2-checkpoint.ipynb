{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d9c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "from torchvision import transforms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d806d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38e4057",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079fe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chemin du dossier de stockage des photo et file.csv de la forme : nom_image, label\n",
    "racine = './panthera_ML_700/photo/300/'\n",
    "df = pd.read_csv(\"./Liste_photos.csv\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Permet de supprimer les quelques images a 1 canaux (noir et blanc) qui peuvent se glisser dans les datasets\n",
    "ind= []  \n",
    "for idx in range(len(df)):\n",
    "    image_path = racine + str(df.iloc[idx][0])\n",
    "    if (Image.open(image_path).getbands() != (\"R\", \"G\", \"B\")):\n",
    "        ind.append(idx)\n",
    "\n",
    "        df = df.drop(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279839b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PantheraDataset:\n",
    "    def __init__(self, df, root_dir, dataType, transform=None):\n",
    "        nbrImg = len(df)\n",
    "        df = df.reset_index(drop=True) \n",
    "        self.data = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label = self.data.iloc[idx][1]\n",
    "        image_path = self.root_dir + str(self.data.iloc[idx][0])\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def checkChannel(self, df):\n",
    "        datasetRGB = []\n",
    "        for index in range(len(df)):\n",
    "            image_path = data_base + str(df.iloc[index][0])\n",
    "            if (Image.open(image_path).getbands() == (\"R\", \"G\", \"B\")): # Check Channels\n",
    "                datasetRGB.append(self.data.iloc[index])\n",
    "        return datasetRGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec94aab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separation des donnees en 3 dataframes : pool, train, test\n",
    "df_pool_init, df_test_init = train_test_split(df, shuffle=True)\n",
    "df_pool_init, df_train_init = train_test_split(df_pool_init, shuffle=True,  test_size=0.055)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d080fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation et chargement des donnees\n",
    "transform = transforms.Compose([transforms.Resize((64, 64)), \n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset_train = PantheraDataset(df_train_init, racine , 'train', transform)\n",
    "dataset_pool = PantheraDataset(df_pool_init, racine , 'pool', transform)\n",
    "dataset_test = PantheraDataset(df_test_init, racine, 'test', transform)\n",
    "\n",
    "#shuffle = false car on a besoin de conserver l'ordre des df pour les codes qu'on ajoute ensuite\n",
    "pre_train_loader = DataLoader(dataset_train, batch_size=64, shuffle=False, num_workers=0)\n",
    "pre_pool_loader = DataLoader(dataset_pool, batch_size=64, shuffle=False, num_workers=0)\n",
    "pre_test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be471329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet : resnet18 pre-entraine auquel on ajoute couche linear pour obtenir en sortie le nbr de classe\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # load modele pre-train\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        #on gele pas les poids de tout les blocs convolutifs\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.features = nn.Sequential(self.resnet.conv1,\n",
    "                                      self.resnet.bn1,\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
    "                                      self.resnet.layer1, \n",
    "                                      self.resnet.layer2, \n",
    "                                      self.resnet.layer3, \n",
    "                                      self.resnet.layer4)\n",
    "        \n",
    "        # average pooling layer\n",
    "        self.avgpool = self.resnet.avgpool\n",
    "\n",
    "        # classifier\n",
    "        self.classifier = self.resnet.fc\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.lin1 = nn.Linear(1000, 512)\n",
    "        self.lin2 = nn.Linear(512, 256)\n",
    "        self.lin3 = nn.Linear(256, 128)\n",
    "        self.lin4 = nn.Linear(128, 64)\n",
    "        self.lin5 = nn.Linear(64, 16)\n",
    "        self.final = nn.Linear(16, 2)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # extract the features\n",
    "        x = self.features(x)\n",
    "        # complete the forward pass\n",
    "        x = self.avgpool(x).squeeze(-1).squeeze(-1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.lin1(x)\n",
    "        dist = self.lin2(x)\n",
    "        x = self.lin3(dist)\n",
    "        x = self.lin4(x)\n",
    "        x = self.lin5(x)\n",
    "        out = self.final(x)\n",
    "        return out, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c434b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreTrain and test sur le pre train pour obtenir les codes de 256 de chaque image\n",
    "def pre_train(num_epoch, model,train_loader, criterion):\n",
    "    model.to(device)\n",
    "    for epoch in range(0, num_epoch):\n",
    "        losses = []\n",
    "        model.train()\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader)) # create a progress bar\n",
    "        for batch_idx, (ancre, lab_ancre) in loop:\n",
    "            out, _ = model(ancre.to(device))\n",
    "            loss = criterion(out, lab_ancre)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, preds_ancre = torch.max(out, 1)\n",
    "\n",
    "            loop.set_description(f\"Epoch {epoch+1}/{num_epoch} process: {int((batch_idx / len(train_loader)) * 100)}\")\n",
    "            loop.set_postfix(loss=loss.data.item())\n",
    "\n",
    "\n",
    "def pre_test(model,dist_list, loaders, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loaders:\n",
    "            output, dist = model(x)\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            correct += (predictions == y).sum().item()\n",
    "            test_loss = criterion(output, y)\n",
    "            dist_list.append(dist)\n",
    "            \n",
    "    test_loss /= len(loaders.dataset)\n",
    "    print(\"Average Loss: \", test_loss, \"  Accuracy: \", correct, \" / \",\n",
    "    len(loaders.dataset), \"  \", int(correct / len(loaders.dataset) * 100), \"%\")\n",
    "    return dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f498d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 process: 75: 100%|██████████| 4/4 [00:42<00:00, 10.53s/it, loss=0.609]\n",
      "Epoch 2/10 process: 75: 100%|██████████| 4/4 [00:42<00:00, 10.58s/it, loss=0.26] \n",
      "Epoch 3/10 process: 75: 100%|██████████| 4/4 [00:44<00:00, 11.11s/it, loss=0.0814]\n",
      "Epoch 4/10 process: 75: 100%|██████████| 4/4 [00:41<00:00, 10.48s/it, loss=0.0574]\n",
      "Epoch 5/10 process: 75: 100%|██████████| 4/4 [00:43<00:00, 11.00s/it, loss=0.0222]\n",
      "Epoch 6/10 process: 75: 100%|██████████| 4/4 [00:46<00:00, 11.60s/it, loss=0.026]\n",
      "Epoch 7/10 process: 75: 100%|██████████| 4/4 [00:42<00:00, 10.58s/it, loss=0.0328]\n",
      "Epoch 8/10 process: 75: 100%|██████████| 4/4 [00:45<00:00, 11.25s/it, loss=0.0422]\n",
      "Epoch 9/10 process: 75: 100%|██████████| 4/4 [00:44<00:00, 11.19s/it, loss=0.0122]\n",
      "Epoch 10/10 process: 75: 100%|██████████| 4/4 [00:46<00:00, 11.74s/it, loss=0.0137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss:  tensor(0.2477)   Accuracy:  8  /  13    61 %\n",
      "Average Loss:  tensor(0.0333)   Accuracy:  53  /  75    70 %\n",
      "Average Loss:  tensor(0.0018)   Accuracy:  195  /  212    91 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.4844,  0.3353, -0.1512,  ...,  0.4865, -0.0757,  0.1967],\n",
       "         [ 2.9913, -0.0513, -1.7463,  ...,  1.9483, -2.6013, -3.2988],\n",
       "         [ 5.3108, -4.7316,  0.2886,  ...,  0.0174, -0.2773,  0.1786],\n",
       "         ...,\n",
       "         [ 2.4015,  2.0412, -1.5819,  ...,  5.2540, -2.3393, -3.1375],\n",
       "         [ 1.1736,  0.4126, -0.8959,  ...,  0.4541, -1.2830, -1.0622],\n",
       "         [-0.9938, -0.8536, -0.6240,  ..., -2.1527,  1.1808,  0.9973]]),\n",
       " tensor([[ 5.0812,  0.3289, -4.8285,  ..., 12.5593, -0.2721, -6.4044],\n",
       "         [ 5.0236,  3.0973, -4.7046,  ...,  5.0923, -1.4490, -5.5336],\n",
       "         [-0.7701, -0.1629,  0.1389,  ..., -1.0981,  0.1059, -0.8461],\n",
       "         ...,\n",
       "         [-0.7925, -0.2425,  0.8790,  ..., -1.4990, -1.2337, -0.3242],\n",
       "         [ 3.9890, -0.0983, -0.9640,  ...,  3.8317, -0.6637, -1.4928],\n",
       "         [-1.8359, -2.5431,  1.0423,  ..., -3.3574,  1.1270,  2.0529]]),\n",
       " tensor([[-1.4510, -1.4672, -0.0529,  ..., -5.8018,  1.6797,  1.9188],\n",
       "         [ 8.3201, -1.0733, -3.1783,  ...,  6.1185,  2.9217, -1.1744],\n",
       "         [10.2012,  1.0705, -4.6036,  ..., 11.5678, -5.6784, -8.0586],\n",
       "         ...,\n",
       "         [ 0.9840,  0.4837, -0.7504,  ...,  0.3132,  0.2142,  0.1137],\n",
       "         [-1.7620, -0.9091,  1.8749,  ..., -2.8053,  0.5708,  1.7333],\n",
       "         [-2.0372, -0.8274,  0.9543,  ..., -1.0917,  1.8972,  3.7116]]),\n",
       " tensor([[-0.6183, -0.9971,  0.3742,  ..., -1.0338, -2.2081, -1.0845],\n",
       "         [ 1.7393,  1.0961, -1.5045,  ...,  0.5436,  0.1336,  0.2029],\n",
       "         [ 5.5709, -0.0755, -5.3090,  ...,  7.2318, -2.7767, -5.0196],\n",
       "         ...,\n",
       "         [ 8.0634,  3.5094, -4.1303,  ..., 12.4751, -3.4235, -6.0592],\n",
       "         [ 3.2477,  1.8226, -3.4559,  ...,  7.2983, -3.1092, -4.6453],\n",
       "         [-1.5813,  0.2262,  1.3467,  ..., -2.2771, -1.8645, -0.4134]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = ResNet().to(device)\n",
    "dist_train = []\n",
    "dist_test = []\n",
    "dist_pool = []\n",
    "num_epochs = 10\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(resnet.parameters(), lr = 0.001) \n",
    "\n",
    "#pre-entrainement\n",
    "pre_train(num_epochs, resnet, pre_pool_loader, loss_func)\n",
    "\n",
    "#recup de l'ensemble des codes de 256 pour entrainer ensuite\n",
    "pre_test( resnet, dist_train, pre_train_loader, loss_func)\n",
    "pre_test( resnet, dist_test, pre_test_loader, loss_func)\n",
    "pre_test( resnet, dist_pool, pre_pool_loader, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d6198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#permet d'ajouter les codes des images au df existant sous forme de 256 colonnes\n",
    "def code(lst_code,df):\n",
    "    code = torch.flatten(lst_code[0], start_dim=1)\n",
    "    for k in range(len(lst_code)-1):\n",
    "        code = torch.cat((code,  torch.flatten(lst_code[k+1], start_dim=1)))\n",
    "\n",
    "    lst_init = list(range(code.shape[0]))\n",
    "\n",
    "    for i in range(code.shape[1]):\n",
    "        df['code'+str(i)] = lst_init  \n",
    "\n",
    "    for i in range(code.shape[0]):  \n",
    "        for j in range(code.shape[1]): #code de 256\n",
    "            df.iloc[i,j+2] = code[i][j].tolist()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c95d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELISE~1.GIL\\AppData\\Local\\Temp/ipykernel_9156/1130764652.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['code'+str(i)] = lst_init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Path  IsVisible     code0     code1     code2     code3  \\\n",
      "25   01250152.JPG          0  0.194604 -0.093778  0.527930 -0.784749   \n",
      "62   04220326.JPG          1  4.392226 -0.233941 -0.659062 -1.945259   \n",
      "100  07010258.JPG          1 -0.083521 -1.326943  0.084026  0.629089   \n",
      "191  08240091.JPG          1  2.335414 -0.606895  0.833315 -2.644835   \n",
      "271  11200845.JPG          1  3.882861  4.429197 -4.145214 -4.299782   \n",
      "..            ...        ...       ...       ...       ...       ...   \n",
      "262  11060485.JPG          1 -4.574972 -3.936323  8.757515  5.885139   \n",
      "110  07100069.JPG          1  1.006188  0.395386 -0.252857  0.884455   \n",
      "137  07280596.JPG          1  1.549986  0.330425 -1.116200 -4.673733   \n",
      "189  08220042.JPG          0 -1.777016 -1.425345  0.699138  0.983488   \n",
      "207  09110349.JPG          1  5.462686  1.361839 -2.986273 -4.050822   \n",
      "\n",
      "        code4     code5     code6     code7  ...   code246   code247  \\\n",
      "25   1.528270 -1.805613  0.306891 -0.450680  ...  2.401562  1.322860   \n",
      "62  -1.835321  1.630880  1.464874  2.561212  ... -3.936032  1.617644   \n",
      "100  0.124326  0.520127  0.778194 -1.196518  ...  3.409276 -0.540803   \n",
      "191 -3.722903  1.648292 -0.613916  3.147391  ... -3.059620 -0.442783   \n",
      "271 -7.959374  4.185420 -2.791897  3.135904  ... -8.557297 -0.406963   \n",
      "..        ...       ...       ...       ...  ...       ...       ...   \n",
      "262  3.789439  1.214133  2.905728 -6.069603  ...  4.092994  7.347941   \n",
      "110 -3.040521  0.649861 -0.506346  2.702378  ... -5.000320 -1.040555   \n",
      "137 -0.169832  1.928391 -0.319219  0.851174  ...  0.411171 -0.283310   \n",
      "189  2.819070 -2.162157  1.083550 -1.338877  ...  2.347040 -0.498906   \n",
      "207 -5.801911  5.296955 -3.183516  6.356965  ... -9.476012  2.765008   \n",
      "\n",
      "      code248   code249    code250   code251   code252   code253   code254  \\\n",
      "25   2.189592  0.933096  -0.576471 -1.116534 -0.747393 -0.692780  0.051429   \n",
      "62  -2.786091  0.354526   3.356733  2.023580 -0.736904  1.351785 -0.668981   \n",
      "100  0.461281 -1.267872  -2.847959  1.420089  0.411705 -1.162662 -0.001309   \n",
      "191 -1.172487  3.675815   3.421944  1.711618 -3.961536  1.868227 -5.188787   \n",
      "271 -7.289730 -1.571623   7.833058  6.232119 -4.702046  6.771842 -2.446931   \n",
      "..        ...       ...        ...       ...       ...       ...       ...   \n",
      "262  8.895156 -3.365643  -2.177414 -1.565448 -0.610876 -5.066280 -5.785134   \n",
      "110 -2.282408 -0.751122   3.538785 -0.134995  0.203087  2.286443 -0.336529   \n",
      "137 -0.470600  3.708868   1.705356  3.020493 -1.559724  0.324372  0.570299   \n",
      "189  3.927044  0.739330  -0.338241 -1.083771 -0.416059 -3.722186 -0.433833   \n",
      "207 -7.588523  2.249768  10.285013  4.626626 -4.946157  7.397022 -2.583398   \n",
      "\n",
      "      code255  \n",
      "25   0.413116  \n",
      "62  -2.172565  \n",
      "100  1.423817  \n",
      "191 -3.484359  \n",
      "271 -4.877726  \n",
      "..        ...  \n",
      "262 -0.177806  \n",
      "110  0.816321  \n",
      "137 -2.190240  \n",
      "189 -1.682664  \n",
      "207 -5.473584  \n",
      "\n",
      "[75 rows x 258 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test_init = code(dist_test, df_test_init)        \n",
    "\n",
    "# code_totest1 = torch.flatten(dist_test[0], start_dim=1)\n",
    "# code_totest2 = torch.flatten(dist_test[1], start_dim=1)\n",
    "# code_totest = torch.cat((code_totest1, code_totest2), 0)\n",
    "                                       \n",
    "# lst_init_test = list(range(code_totest.shape[0]))\n",
    "\n",
    "# for i in range(code_totest.shape[1]):\n",
    "#     df_test_init['code'+str(i)] = lst_init_test  \n",
    "\n",
    "# for i in range(code_totest.shape[0]):  \n",
    "#     for j in range(code_totest.shape[1]): #code de 256\n",
    "#         df_test_init.iloc[i,j+2] = code_totest[i][j].tolist()\n",
    "\n",
    "print(df_test_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e1580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELISE~1.GIL\\AppData\\Local\\Temp/ipykernel_9156/1130764652.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['code'+str(i)] = lst_init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Path  IsVisible     code0     code1     code2     code3  \\\n",
      "241  10240343.JPG          1  0.484419  0.335283 -0.151161 -1.755296   \n",
      "28   01270982.JPG          1  2.991263 -0.051276 -1.746266 -1.090038   \n",
      "26   01270079.JPG          1  5.310846 -4.731625  0.288645  4.969981   \n",
      "239  10230095.JPG          0 -1.562689 -0.758013  0.413579  1.009370   \n",
      "72   05070233.JPG          0 -2.977506 -0.013212  2.317851  1.280113   \n",
      "..            ...        ...       ...       ...       ...       ...   \n",
      "42   02230154.JPG          0  1.451494  2.036175 -3.738444 -5.443411   \n",
      "188  08210992.JPG          1  6.325446  2.270733 -3.732398 -2.075155   \n",
      "265  11110740.JPG          1  8.063412  3.509393 -4.130260 -7.020498   \n",
      "73   05120985.JPG          1  3.247664  1.822605 -3.455887 -3.850223   \n",
      "199  09020078.JPG          0 -1.581332  0.226238  1.346731 -0.304183   \n",
      "\n",
      "         code4     code5     code6     code7  ...    code246   code247  \\\n",
      "241  -0.554320 -1.803797  0.013066  0.251556  ...  -0.102534  0.361669   \n",
      "28   -0.904317  1.281428 -1.968907  1.756791  ...  -1.652207  1.268597   \n",
      "26    0.142681  4.332126 -0.970307  0.216401  ...   1.341498  1.005325   \n",
      "239   1.689168 -0.909424  1.308272 -1.071813  ...   2.316193 -0.135658   \n",
      "72    1.220246 -2.019339  1.827656 -1.127173  ...   3.078037  0.607457   \n",
      "..         ...       ...       ...       ...  ...        ...       ...   \n",
      "42    1.410391 -1.329951 -5.637558  2.091981  ...   4.134476 -2.773631   \n",
      "188  -4.031199  4.112123 -5.144421  4.549088  ...  -6.435086  0.277752   \n",
      "265 -11.386741  4.087392 -3.959953  9.910343  ... -11.183212  1.815414   \n",
      "73   -3.678572  2.146337 -5.066620  3.670249  ...  -8.764094  1.897566   \n",
      "199   1.402047 -1.632459  0.212795 -3.292675  ...   1.739407  1.198275   \n",
      "\n",
      "       code248   code249    code250    code251   code252    code253   code254  \\\n",
      "241  -0.528747  1.204255  -0.445739   0.433531 -0.050395   0.486539 -0.075724   \n",
      "28   -1.739384  0.420389   3.895957   1.844945 -1.947130   1.948337 -2.601332   \n",
      "26    4.258509 -6.769332  -1.909860   0.977361  1.632283   0.017429 -0.277258   \n",
      "239   2.398029 -0.772538  -1.190380  -1.393503  1.590700  -2.611738  0.563038   \n",
      "72    2.198448 -1.506134  -2.880033  -0.499743  3.542107  -3.117755  1.920419   \n",
      "..         ...       ...        ...        ...       ...        ...       ...   \n",
      "42   -1.335105  6.002421   0.688622   1.287944 -5.126486   1.244660 -1.023260   \n",
      "188  -5.922347  0.743044   6.172862   4.858385 -2.851052   6.079863 -1.249606   \n",
      "265 -11.339411  1.563925  13.553761  10.040733 -2.391405  12.475059 -3.423530   \n",
      "73   -5.517518  0.708944   8.699977   1.028304 -4.494159   7.298331 -3.109165   \n",
      "199   1.620796  1.729461  -3.212581  -3.682472  0.407466  -2.277116 -1.864498   \n",
      "\n",
      "      code255  \n",
      "241  0.196664  \n",
      "28  -3.298832  \n",
      "26   0.178614  \n",
      "239  1.616638  \n",
      "72   4.397960  \n",
      "..        ...  \n",
      "42  -7.198591  \n",
      "188 -4.103334  \n",
      "265 -6.059191  \n",
      "73  -4.645299  \n",
      "199 -0.413417  \n",
      "\n",
      "[212 rows x 258 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pool_init = code(dist_pool, df_pool_init)        \n",
    "\n",
    "# code_topool1 = torch.flatten(dist_pool[0], start_dim=1)\n",
    "# code_topool2 = torch.flatten(dist_pool[1], start_dim=1)\n",
    "# code_topool3 = torch.flatten(dist_pool[2], start_dim=1)\n",
    "# code_topool4 = torch.flatten(dist_pool[3], start_dim=1)\n",
    "\n",
    "# code_topool = torch.cat((code_topool1, code_topool2, code_topool3, code_topool4), 0)\n",
    "                                       \n",
    "# lst_init_pool = list(range(code_topool.shape[0]))\n",
    "\n",
    "# for i in range(code_topool.shape[1]):\n",
    "#     df_pool_init['code'+str(i)] = lst_init_pool  \n",
    "\n",
    "# for i in range(code_topool.shape[0]):  \n",
    "#     for j in range(code_topool.shape[1]): #code de 256\n",
    "#         df_pool_init.iloc[i,j+2] = code_topool[i][j].tolist()\n",
    "\n",
    "print(df_pool_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3569f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELISE~1.GIL\\AppData\\Local\\Temp/ipykernel_9156/241561601.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train_init['code'+str(i)] = lst_init_train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Path  IsVisible      code0     code1     code2     code3  \\\n",
      "299  IMG_3483.JPG          0  -2.705439 -1.957094  2.135251  2.329598   \n",
      "280  12160185.JPG          0  -3.412981 -0.771653  0.191533 -0.235894   \n",
      "32   02010695.JPG          0  -1.205842  2.504037 -0.350442  0.791979   \n",
      "150  08040284.JPG          0  -3.822272 -0.065354  1.854468  0.240487   \n",
      "275  12070027.JPG          0   0.453437 -0.106201 -0.886208 -1.496242   \n",
      "63   04290432.JPG          0  10.897120  2.275590 -5.506807 -8.002598   \n",
      "282  EK001077.JPG          1   1.519726  0.461225 -1.514726 -1.162420   \n",
      "253  11040324.JPG          1  -0.850972  3.544873  1.216276 -1.381055   \n",
      "84   06060412.JPG          1  -4.056543 -1.126324  3.337926  2.578945   \n",
      "99   06300937.JPG          1   0.415354 -0.165483  0.498155  0.301434   \n",
      "30   02010692.JPG          1   3.143299  0.088998 -1.101792 -2.241890   \n",
      "248  11020267.JPG          0  -0.526440 -1.591881  1.524948  1.650377   \n",
      "103  07040765.JPG          1  -1.862318 -1.094393  2.306413  1.273561   \n",
      "\n",
      "        code4     code5     code6     code7  ...   code246   code247  \\\n",
      "299  2.705554 -1.001793  2.914593 -2.157184  ...  2.678928 -0.399043   \n",
      "280 -0.732047 -2.382448  2.141179 -0.506945  ...  2.630096  0.448544   \n",
      "32  -2.368429  1.793673  2.118027  0.529349  ... -7.057899  0.375433   \n",
      "150  1.674714 -2.606217  1.167414 -2.308958  ...  3.896896 -1.154711   \n",
      "275  0.218484  0.794701 -0.252127  1.850093  ... -1.984929  0.407152   \n",
      "63  -4.767617  3.441682 -4.667129  8.994762  ... -8.345773  1.684658   \n",
      "282 -1.088019  1.024792 -0.815602  0.718859  ... -0.978135  0.335067   \n",
      "253 -0.854668 -0.224248 -5.084030  2.333602  ... -0.806424  2.976773   \n",
      "84   2.077071 -3.353751  3.110362 -3.598589  ...  2.745529  0.251267   \n",
      "99   0.263141 -0.904167 -0.219355  0.059532  ... -0.419946  0.463598   \n",
      "30  -3.711312  3.486890 -1.804756  3.820646  ... -6.319835  2.366866   \n",
      "248 -0.088193 -0.317646  3.282997 -0.471962  ... -0.392820  2.005185   \n",
      "103  1.007049 -1.212826  1.968509 -1.965380  ... -0.155119  0.847941   \n",
      "\n",
      "      code248   code249    code250   code251   code252    code253   code254  \\\n",
      "299  4.245693 -0.159816  -3.949553 -1.373145  2.561993  -3.465663  0.601403   \n",
      "280  1.212054 -0.103035  -2.151273 -2.009697  0.522108  -3.180448 -0.401662   \n",
      "32  -4.396321 -4.593919   5.145515  2.207765  0.513674   1.320970 -0.772023   \n",
      "150  4.102610  0.898266  -2.972354 -2.106374  1.379467  -5.773404  0.883548   \n",
      "275 -0.207793  2.463642   1.575218  0.407782 -3.000671   1.263235 -0.606273   \n",
      "63  -7.746771  7.632177  10.370946  6.822309 -7.608311  11.114820 -2.704127   \n",
      "282 -0.109529  1.033171   1.391475  1.642865 -1.249331   0.519855 -0.137602   \n",
      "253 -0.054423  1.756546  -0.657978 -0.006231 -3.635552   4.583692 -3.529521   \n",
      "84   3.313310 -2.708004  -3.324048 -3.277256  2.733106  -4.474720  1.490212   \n",
      "99   0.065143 -0.103692   0.291766  0.554132 -0.454995  -0.493746  0.043464   \n",
      "30  -2.372660  2.114387   5.400913  2.435821 -3.900608   3.607972 -2.230699   \n",
      "248  0.642326 -3.181848  -1.002850 -0.265352  0.404266  -1.683347  2.166196   \n",
      "103  1.831649 -1.441002  -2.066738 -1.707103  1.747819  -3.385036  0.712844   \n",
      "\n",
      "      code255  \n",
      "299  1.485346  \n",
      "280  0.501299  \n",
      "32   0.621962  \n",
      "150  1.706565  \n",
      "275 -1.237133  \n",
      "63  -7.656292  \n",
      "282 -1.557442  \n",
      "253 -2.145248  \n",
      "84   3.062448  \n",
      "99  -0.063226  \n",
      "30  -3.241877  \n",
      "248  1.982538  \n",
      "103  1.859358  \n",
      "\n",
      "[13 rows x 258 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(dist_train))\n",
    "code_totrain = torch.flatten(dist_train[0], start_dim=1)\n",
    "code_totrain=torch.cat((dist_train), 0)                            \n",
    "lst_init_train = list(range(code_totrain.shape[0]))\n",
    "\n",
    "for i in range(code_totrain.shape[1]):\n",
    "    df_train_init['code'+str(i)] = lst_init_train  \n",
    "\n",
    "for i in range(code_totrain.shape[0]):  \n",
    "    for j in range(code_totrain.shape[1]): #code de 256\n",
    "        df_train_init.iloc[i,j+2] = code_totrain[i][j].tolist()\n",
    "\n",
    "print(df_train_init)\n",
    "#df_train_init = code(dist_train, df_train_init)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea50a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(modele, y_test, X_test):\n",
    "    #Prediction sur le Test set\n",
    "    accuracy=0\n",
    "    y_pred = modele.predict(X_test)\n",
    "    accuracy += (y_pred == y_test).sum().item() *100\n",
    "    accuracy /= len(X_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d54afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Active learning : ajout des données en fonction de l'entropie\n",
    "def entropy(modele, df_pool, df_train, name):\n",
    "    entropy_tensor = []\n",
    "    df_pool = df_pool.reset_index(drop=True)\n",
    "    test_output = modele.predict_proba(df_pool.iloc[:,2:].values.tolist())\n",
    "    entrop = (-(torch.from_numpy(test_output+10**-7))*torch.log(torch.from_numpy(test_output+10**-7))).sum(1)\n",
    "    entropy_tensor.append(entrop.tolist())\n",
    "\n",
    "    entropy = [item for l in entropy_tensor for item in l] #conversion tensor en liste\n",
    "\n",
    "    df_pool[\"entrop\"]=entropy\n",
    "    df_pool = df_pool.sort_values(by=['entrop'], ascending=False)\n",
    "    \n",
    "\n",
    "    df_max_entrop = df_pool.head(5)\n",
    "    name=pd.concat([name, df_max_entrop['Path']], axis=0, ignore_index=True) \n",
    "\n",
    "    #name = name['Name']\n",
    "    df_pool=df_pool.drop([0, 1, 2, 3, 4])\n",
    "    del df_pool['entrop']\n",
    "    df_train = pd.concat([df_train, df_max_entrop], axis=0, ignore_index=True)\n",
    "    return [df_pool, df_train, name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4a09332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random : ajout des données aléatoirement\n",
    "def alea_choice(df_pool, df_train, name):\n",
    "    df_pool = df_pool.reset_index(drop=True)\n",
    "\n",
    "    random_numbers = random.sample(range(len(df_pool)), k=5)\n",
    "    df_random= df_pool.iloc[random_numbers]\n",
    "    df_pool=df_pool.drop(random_numbers)\n",
    "    name=pd.concat([name, df_random['Path']], axis=0, ignore_index=True) \n",
    "    df_train = pd.concat([df_train, df_random], axis=0, ignore_index=True)\n",
    "    return[df_pool, df_train, name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actlearn(df_train_bis, df_pool_bis, df_test_bis, seeds):\n",
    "    acc = []\n",
    "    name = pd.DataFrame()\n",
    "    while(True):\n",
    "        df_train_bis.iloc[:, 2:258] = np.nan_to_num(df_train_bis.iloc[:, 2:258])\n",
    "        mod = RandomForestClassifier(max_depth=3, random_state=seeds)\n",
    "        #mod = DecisionTreeClassifier(random_state=0,  max_depth=2, splitter=\"random\")\n",
    "        mod.fit(df_train_bis.iloc[:, 2:258].values, df_train_bis.iloc[:,1].values)\n",
    "        \n",
    "        #all_labels, all_preds = test(cnn)\n",
    "        acc.append(test(mod, df_test_bis.iloc[:, 1].values, df_test_bis.iloc[:, 2:].values))\n",
    "        \n",
    "        df_pool_bis , df_train_bis, name= entropy(mod, df_pool_bis, df_train_bis, name)\n",
    "\n",
    "        if ((len(df_pool_bis)<10)):\n",
    "            break\n",
    "    return acc, name\n",
    "\n",
    "def randomlearn(df_train_bis, df_pool_bis, df_test_bis, seeds):\n",
    "    acc = []\n",
    "    name = pd.DataFrame()\n",
    "    while(True):\n",
    "        df_train_bis.iloc[:, 2:258] = np.nan_to_num(df_train_bis.iloc[:, 2:258])\n",
    "        #mod = DecisionTreeClassifier(random_state=0,  max_depth=2, splitter=\"random\")\n",
    "        mod = RandomForestClassifier(max_depth=3, random_state=seeds)\n",
    "\n",
    "        mod.fit(df_train_bis.iloc[:, 2:258].values, df_train_bis.iloc[:,1].values)\n",
    "        #all_labels, all_preds = test(cnn)\n",
    "        acc.append(test(mod, df_test_bis.iloc[:, 1].values, df_test_bis.iloc[:, 2:].values))\n",
    "        \n",
    "        df_pool_bis , df_train_bis, name = alea_choice(df_pool_bis , df_train_bis, name)\n",
    "\n",
    "        if ((len(df_pool_bis)<10)):\n",
    "            break\n",
    "    return acc, name\n",
    "\n",
    "accuracy_act = []\n",
    "accuracy_random = []\n",
    "seeds = np.arange(10)\n",
    "for i in range(10):\n",
    "    df_train_bis = df_train_init\n",
    "    df_pool_bis = df_pool_init\n",
    "    df_test_bis = df_test_init\n",
    "    acc_al, df_name_al = actlearn(df_train_bis, df_pool_bis, df_test_bis, seeds[i])\n",
    "   # accuracy_act.append(actlearn(df_train_bis, df_pool_bis, df_test_bis, seeds[i])[0])\n",
    "    accuracy_act.append(acc_al)\n",
    "    df_train_bis = df_train_init\n",
    "    df_pool_bis = df_pool_init\n",
    "    df_test_bis = df_test_init\n",
    "    acc_rl, df_name_rl = randomlearn(df_train_bis, df_pool_bis, df_test_bis, seeds[i])\n",
    "    accuracy_random.append(acc_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_moy_act = []\n",
    "acc_act_min = []\n",
    "acc_act_max = []\n",
    "for i in range(len(accuracy_act[0])):\n",
    "    acc_act_min.append(100000)\n",
    "    acc_act_max.append(-1)\n",
    "\n",
    "for i in range(len(accuracy_act[0])):\n",
    "    moy = 0\n",
    "    for j in range(len(accuracy_act)):\n",
    "        moy += accuracy_act[j][i]\n",
    "        if (accuracy_act[j][i] > acc_act_max[i]):\n",
    "            acc_act_max[i] = accuracy_act[j][i]\n",
    "        if (accuracy_act[j][i] < acc_act_min[i]):\n",
    "            acc_act_min[i] = accuracy_act[j][i]    \n",
    "            \n",
    "    acc_moy_act.append(moy/len(accuracy_act))\n",
    "        \n",
    "acc_moy_random = []\n",
    "acc_random_max = []\n",
    "acc_random_min = []\n",
    "for i in range(len(accuracy_act[0])):\n",
    "    acc_random_min.append(100000)\n",
    "    acc_random_max.append(-1)\n",
    "    \n",
    "for i in range(len(accuracy_random[0])):\n",
    "    moy = 0\n",
    "    for j in range(len(accuracy_random)):\n",
    "        moy += accuracy_random[j][i]\n",
    "        if (accuracy_random[j][i] > acc_random_max[i]):\n",
    "            acc_random_max[i] = accuracy_random[j][i]\n",
    "        if (accuracy_random[j][i] < acc_random_min[i]):\n",
    "            acc_random_min[i] = accuracy_random[j][i]  \n",
    "    acc_moy_random.append(moy/len(accuracy_random))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e79e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file csv with high low means\n",
    "df_al = pd.DataFrame(list(zip(acc_act_max, acc_act_min, acc_moy_act)), columns =['high', 'low', 'mean'])\n",
    "df_rl = pd.DataFrame(list(zip(acc_random_max, acc_random_min, acc_moy_random)), columns =['high', 'low', 'mean'])\n",
    "\n",
    "df_al.to_csv('al.csv', index=False)\n",
    "df_rl.to_csv('rl.csv', index=False)\n",
    "df_name_al.to_csv('name_al.csv', index=False)\n",
    "df_name_rl.to_csv('name_rl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage des courbes voulues\n",
    "plt.figure (figsize= (10,5))\n",
    "x = np.linspace(0, len(acc_moy_act), len(acc_moy_act))*5\n",
    "plt.plot(x, acc_moy_act, label='active accuracy', color='r')\n",
    "plt.plot(x, acc_moy_random, label='random accuracy', color='b')\n",
    "# plt.plot(x, acc_random_max, label='random accuracy', color='b')\n",
    "# plt.plot(x, acc_random_min, label=' random accuracy', color='b')\n",
    "# plt.plot(x, acc_act_min, label='act accuracy  ', color='r')\n",
    "# plt.plot(x, acc_act_max, label='act accuracy', color='r')\n",
    "\n",
    "plt.fill_between(x, acc_act_min, acc_act_max, color='tomato', alpha=0.25)\n",
    "\n",
    "plt.fill_between(x, acc_random_min, acc_random_max, color='#539ecd',  alpha=0.25)\n",
    "\n",
    "plt.title(\"Evolution de la précision en fonction des ajouts d'audio dans le train\")\n",
    "plt.ylabel(\"Précision du modèle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4f051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
